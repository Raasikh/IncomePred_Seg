{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc977b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 1: Data Exploration & Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a468f919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dc9d063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns: 42\n",
      "['age', 'class of worker', 'detailed industry recode', 'detailed occupation recode', 'education', 'wage per hour', 'enroll in edu inst last wk', 'marital stat', 'major industry code', 'major occupation code', 'race', 'hispanic origin', 'sex', 'member of a labor union', 'reason for unemployment', 'full or part time employment stat', 'capital gains', 'capital losses', 'dividends from stocks', 'tax filer stat', 'region of previous residence', 'state of previous residence', 'detailed household and family stat', 'detailed household summary in household', 'weight', 'migration code-change in msa', 'migration code-change in reg', 'migration code-move within reg', 'live in this house 1 year ago', 'migration prev res in sunbelt', 'num persons worked for employer', 'family members under 18', 'country of birth father', 'country of birth mother', 'country of birth self', 'citizenship', 'own business or self employed', \"fill inc questionnaire for veteran's admin\", 'veterans benefits', 'weeks worked in year', 'year', 'label']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Read column names\n",
    "with open('../data/census-bureau.columns', 'r') as f:\n",
    "    columns = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "print(f\"Total columns: {len(columns)}\")\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72644760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: 199,523 rows x 42 columns\n",
      "Memory: 433.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset with low_memory=False to avoid mixed type issues\n",
    "df = pd.read_csv('../data/census-bureau.data', header=None, names=columns, low_memory=False)\n",
    "\n",
    "# Strip whitespace from all string columns\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'str' or df[col].dtype == 'object':\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "# Fix the ‹73 value in age and convert to numeric\n",
    "df['age'] = df['age'].replace('‹73', '73')\n",
    "df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "\n",
    "print(f\"Shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "print(f\"Memory: {df.memory_usage(deep=True).sum() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf8d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the data\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb34ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66237d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f450ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2619f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column overview\n",
    "summary = pd.DataFrame({\n",
    "    'dtype': df.dtypes,\n",
    "    'unique_values': df.nunique(),\n",
    "    'null_count': df.isnull().sum(),\n",
    "    'sample': df.iloc[0]\n",
    "})\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d46a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric vs categorical columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = [col for col in df.columns if df[col].dtype == 'str' or df[col].dtype == 'object']\n",
    "\n",
    "print(f\"NUMERIC COLUMNS ({len(numeric_cols)}):\")\n",
    "for c in numeric_cols:\n",
    "    print(f\"  - {c} | unique={df[c].nunique()} | range=[{df[c].min()}, {df[c].max()}]\")\n",
    "\n",
    "print(f\"\\nCATEGORICAL COLUMNS ({len(categorical_cols)}):\")\n",
    "for c in categorical_cols:\n",
    "    print(f\"  - {c} | unique={df[c].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95779976",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique label values:\")\n",
    "print(df['label'].unique())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23abcda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df['label'].value_counts()\n",
    "for val, count in label_counts.items():\n",
    "    print(f'  \"{val}\": {count:,} ({count/len(df)*100:.2f}%)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa2dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count NIU and ? across all columns\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'str' or df[col].dtype == 'object':\n",
    "        niu = (df[col].str.contains('Not in universe', case=False, na=False)).sum()\n",
    "        q = (df[col] == '?').sum()\n",
    "        if niu > 0 or q > 0:\n",
    "            print(f\"{col:<45s} NIU: {niu:>7,} ({niu/len(df)*100:>5.1f}%)   ?: {q:>7,} ({q/len(df)*100:>5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d241319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Zero analysis\n",
    "for col in numeric_cols:\n",
    "    zeros = (df[col] == 0).sum()\n",
    "    print(f\"  {col:<35s} min={df[col].min():<10} max={df[col].max():<10} zeros={zeros:>7,} ({zeros/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e33796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values for every categorical column\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{col} ({df[col].nunique()} unique values)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    vc = df[col].value_counts()\n",
    "    for val, count in vc.items():\n",
    "        pct = count/len(df)*100\n",
    "        bar = '█' * int(pct / 2)\n",
    "        print(f'  \"{val}\": {count:>8,} ({pct:>5.1f}%) {bar}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Categorical columns summary\n",
    "string_cols = [col for col in df.columns if df[col].dtype == 'str' or df[col].dtype == 'object']\n",
    "print(f\"Categorical columns ({len(string_cols)}):\\n\")\n",
    "for col in string_cols:\n",
    "    n_unique = df[col].nunique()\n",
    "    top_val = df[col].value_counts().index[0]\n",
    "    top_pct = df[col].value_counts().iloc[0] / len(df) * 100\n",
    "    print(f\"  {col:<45s} unique={n_unique:<5} top: '{top_val}' ({top_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b0b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary target for analysis\n",
    "df['income_binary'] = df['label'].apply(lambda x: 1 if '50000+' in str(x) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78670e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income rate by education\n",
    "print(\"=== EDUCATION vs INCOME ===\")\n",
    "ct = df.groupby('education')['income_binary'].mean().sort_values(ascending=False)\n",
    "for edu, rate in ct.items():\n",
    "    bar = '█' * int(rate * 50)\n",
    "    print(f\"  {edu:<40s} {rate*100:>5.1f}% {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34552429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income rate by class of worker\n",
    "print(\"=== CLASS OF WORKER vs INCOME ===\")\n",
    "ct = df.groupby('class of worker')['income_binary'].mean().sort_values(ascending=False)\n",
    "for val, rate in ct.items():\n",
    "    bar = '█' * int(rate * 50)\n",
    "    print(f\"  {val:<45s} {rate*100:>5.1f}% {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a57bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income rate by age groups\n",
    "print(\"=== AGE GROUP vs INCOME ===\")\n",
    "df['age_group'] = pd.cut(df['age'], bins=[0, 18, 30, 45, 60, 100], labels=['0-18', '19-30', '31-45', '46-60', '61+'])\n",
    "ct = df.groupby('age_group')['income_binary'].mean()\n",
    "for val, rate in ct.items():\n",
    "    bar = '█' * int(rate * 50)\n",
    "    print(f\"  {str(val):<15s} {rate*100:>5.1f}% {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e51bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Income rate by sex\n",
    "print(\"=== SEX vs INCOME ===\")\n",
    "ct = df.groupby('sex')['income_binary'].mean().sort_values(ascending=False)\n",
    "for val, rate in ct.items():\n",
    "    bar = '█' * int(rate * 50)\n",
    "    print(f\"  {val:<45s} {rate*100:>5.1f}% {bar}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71927b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income rate by major industry and occupation\n",
    "for col_name in ['major industry code', 'major occupation code', 'tax filer stat']:\n",
    "    ct = df.groupby(col_name)['income_binary'].mean().sort_values(ascending=False)\n",
    "    print(f\"\\n=== {col_name} ===\")\n",
    "    for val, rate in ct.items():\n",
    "        print(f\"  {val:<50s} {rate*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11eb4fa",
   "metadata": {},
   "source": [
    "# ## Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with >90% NIU or >50% ? values\n",
    "drop_cols = [\n",
    "    'enroll in edu inst last wk',           # 93.7% NIU\n",
    "    'member of a labor union',              # 90.4% NIU\n",
    "    'reason for unemployment',              # 97.0% NIU\n",
    "    'region of previous residence',         # 92.1% NIU\n",
    "    'state of previous residence',          # 92.1% NIU\n",
    "    'fill inc questionnaire for veteran\\'s admin',  # 99.0% NIU\n",
    "    'migration code-change in msa',         # 50% ?\n",
    "    'migration code-change in reg',         # 50% ?\n",
    "    'migration code-move within reg',       # 50% ?\n",
    "    'migration prev res in sunbelt',        # 42% NIU + 50% ?\n",
    "]\n",
    "\n",
    "print(f\"Dropping {len(drop_cols)} columns:\")\n",
    "for c in drop_cols:\n",
    "    print(f\"  - {c}\")\n",
    "\n",
    "df.drop(columns=drop_cols, inplace=True)\n",
    "print(f\"\\nShape after dropping: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e2dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Drop hispanic origin (redundant with race) and detailed household family stat (38 unique, have simpler version)\n",
    "df.drop(columns=['hispanic origin', 'detailed household and family stat'], inplace=True)\n",
    "print(f\"Shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb741e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify country of birth columns to US vs Non-US\n",
    "for col in ['country of birth father', 'country of birth mother', 'country of birth self']:\n",
    "    df[col] = df[col].apply(lambda x: 'US' if x == 'United-States' else 'Non-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b4396",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['country of birth father', 'country of birth mother', 'country of birth self']:\n",
    "    print(f\"{col}: {df[col].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46399f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables using Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_cols = [col for col in df.columns if df[col].dtype == 'str' or df[col].dtype == 'object']\n",
    "cat_cols = [c for c in cat_cols if c not in ['label', 'income_binary', 'age_group']]\n",
    "\n",
    "print(f\"Encoding {len(cat_cols)} columns:\")\n",
    "\n",
    "label_encoders = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"  {col:<45s} -> {df[col].nunique()} values\")\n",
    "\n",
    "print(f\"\\nDone. All columns are now numeric.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c9631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features, target, and weights\n",
    "exclude = ['label', 'income_binary', 'weight', 'age_group', 'year']\n",
    "\n",
    "X = df.drop(columns=exclude)\n",
    "y = df['income_binary']\n",
    "w = df['weight']\n",
    "\n",
    "print(f\"Features: {X.shape}\")\n",
    "print(f\"Target: {y.shape} | >50K: {y.sum():,} ({y.mean()*100:.1f}%)\")\n",
    "print(f\"Feature columns: {X.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8247058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(\n",
    "    X, y, w,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape} | >50K: {y_train.mean()*100:.1f}%\")\n",
    "print(f\"Test:  {X_test.shape} | >50K: {y_test.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ef1110",
   "metadata": {},
   "source": [
    "# ## Step 3: Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b92ee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, roc_curve\n",
    "\n",
    "# Scale features for logistic regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_scaled, y_train, sample_weight=w_train)\n",
    "\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "y_prob_lr = lr.predict_proba(X_test_scaled)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf99a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=== Logistic Regression Results ===\\n\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=['<=50K', '>50K']))\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_prob_lr, sample_weight=w_test):.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),\n",
    "    random_state=42,\n",
    "    eval_metric='aucpr'\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train, sample_weight=w_train)\n",
    "\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "y_prob_xgb = xgb.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ed5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== XGBoost Results ===\\n\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=['<=50K', '>50K']))\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_prob_xgb, sample_weight=w_test):.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334af063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'scale_pos_weight': len(y_train[y_train==0]) / len(y_train[y_train==1]),\n",
    "        'random_state': 42,\n",
    "        'eval_metric': 'aucpr'\n",
    "    }\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train, sample_weight=w_train)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    return roc_auc_score(y_test, y_prob, sample_weight=w_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72b2b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(f\"Best ROC AUC: {study.best_value:.4f}\")\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for key, val in study.best_params.items():\n",
    "    print(f\"  {key}: {val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5100a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "best_params['scale_pos_weight'] = len(y_train[y_train==0]) / len(y_train[y_train==1])\n",
    "best_params['random_state'] = 42\n",
    "best_params['eval_metric'] = 'aucpr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_final = XGBClassifier(**best_params)\n",
    "xgb_final.fit(X_train, y_train, sample_weight=w_train)\n",
    "\n",
    "y_pred_final = xgb_final.predict(X_test)\n",
    "y_prob_final = xgb_final.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88faaee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Final XGBoost Results ===\\n\")\n",
    "print(classification_report(y_test, y_pred_final, target_names=['<=50K', '>50K']))\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_prob_final, sample_weight=w_test):.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = xgb_final.feature_importances_\n",
    "feat_imp = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 Features:\")\n",
    "for feat, imp in feat_imp.head(10).items():\n",
    "    bar = '█' * int(imp * 100)\n",
    "    print(f\"  {feat:<35s} {imp:.4f} {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623af1e8",
   "metadata": {},
   "source": [
    "# ## Step 4: Customer Segmentation (K-Means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select segmentation features\n",
    "seg_numeric = ['age', 'capital gains', 'capital losses', 'dividends from stocks',\n",
    "               'weeks worked in year', 'wage per hour']\n",
    "\n",
    "seg_categorical = ['sex', 'education', 'marital stat', 'class of worker',\n",
    "                   'country of birth self']\n",
    "\n",
    "# Get numeric features directly\n",
    "seg_data = df[seg_numeric].copy()\n",
    "\n",
    "# One-hot encode categorical features\n",
    "for col in seg_categorical:\n",
    "    dummies = pd.get_dummies(df[col], prefix=col, dtype=int)\n",
    "    seg_data = pd.concat([seg_data, dummies], axis=1)\n",
    "\n",
    "print(f\"Segmentation feature shape: {seg_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abfbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "scaler_seg = StandardScaler()\n",
    "seg_scaled = scaler_seg.fit_transform(seg_data)\n",
    "\n",
    "print(f\"Scaled shape: {seg_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cd731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method to find optimal k\n",
    "inertias = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(seg_scaled)\n",
    "    inertias.append(km.inertia_)\n",
    "    print(f\"  k={k}: inertia={km.inertia_:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d79786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Fit final K-Means with k=5\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "df['cluster'] = kmeans.fit_predict(seg_scaled)\n",
    "\n",
    "print(\"Cluster distribution:\")\n",
    "print(df['cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc24b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/census-bureau.columns', 'r') as f:\n",
    "    columns_orig = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "df_orig = pd.read_csv('../data/census-bureau.data', header=None, names=columns_orig, low_memory=False)\n",
    "for col in df_orig.columns:\n",
    "    if df_orig[col].dtype == 'str' or df_orig[col].dtype == 'object':\n",
    "        df_orig[col] = df_orig[col].astype(str).str.strip()\n",
    "df_orig['age'] = pd.to_numeric(df_orig['age'].replace('‹73', '73'), errors='coerce')\n",
    "df_orig['cluster'] = df['cluster']\n",
    "df_orig['income_binary'] = df['income_binary']\n",
    "\n",
    "# Profile each cluster\n",
    "profile_cols = ['age', 'sex', 'education', 'marital stat', 'class of worker',\n",
    "                'capital gains', 'weeks worked in year', 'wage per hour']\n",
    "\n",
    "for k in range(5):\n",
    "    cluster_data = df_orig[df_orig['cluster'] == k]\n",
    "    pct = len(cluster_data) / len(df_orig) * 100\n",
    "    income_rate = cluster_data['income_binary'].mean() * 100\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLUSTER {k} | Size: {len(cluster_data):,} ({pct:.1f}%) | >50K: {income_rate:.1f}%\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Age\n",
    "    print(f\"  Age: mean={cluster_data['age'].mean():.0f}, median={cluster_data['age'].median():.0f}\")\n",
    "    \n",
    "    # Categorical features - top 2 values\n",
    "    for col in ['sex', 'education', 'marital stat', 'class of worker']:\n",
    "        top = cluster_data[col].value_counts().head(2)\n",
    "        print(f\"  {col}:\")\n",
    "        for val, count in top.items():\n",
    "            print(f\"    - {val}: {count/len(cluster_data)*100:.1f}%\")\n",
    "    \n",
    "    # Numeric means\n",
    "    print(f\"  Weeks worked: {cluster_data['weeks worked in year'].mean():.1f}\")\n",
    "    print(f\"  Capital gains: {cluster_data['capital gains'].mean():.0f}\")\n",
    "    print(f\"  Wage/hour: {cluster_data['wage per hour'].mean():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1070b",
   "metadata": {},
   "source": [
    "# ## Summary\n",
    "# \n",
    "# **Classification:** XGBoost with Optuna tuning achieved ROC-AUC of ~0.956\n",
    "# \n",
    "# **Segmentation:** 5 clusters identified:\n",
    "# - Cluster 0: Working Professionals (41.3%, 12.1% >50K)\n",
    "# - Cluster 1: Associate Degree Holders (2.2%, 9.4% >50K)\n",
    "# - Cluster 2: Part-Time/Underemployed (11.1%, 5.5% >50K)\n",
    "# - Cluster 3: Children/Minors (28.0%, 0.0% >50K)\n",
    "# - Cluster 4: Retirees (17.4%, 2.2% >50K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bcfc61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
